{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all necessary libraries...\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.read_csv(\"./archive/creditcard.csv\").astype('float32')\n",
    "\n",
    "# preprocess Data\n",
    "Data = Data.drop(['Time'], axis=1)\n",
    "Data['Amount'] = StandardScaler().fit_transform(Data['Amount'].values.reshape(-1,1))\n",
    "\n",
    "fraud = Data[Data['Class'] == 1]\n",
    "nonFraud = Data[Data['Class'] == 0]\n",
    "\n",
    "# seperate nonfraud data \n",
    "trainData, testData = train_test_split(nonFraud, train_size=0.90)\n",
    "\n",
    "# add fraud data to testData\n",
    "testData = pd.concat([fraud, testData])\n",
    "\n",
    "# final preprocessing\n",
    "trainData = trainData.drop(['Class'], axis=1)\n",
    "\n",
    "# save both testData and trainData for future use\n",
    "trainData.to_csv('./Dataset/trainingData', index=False)\n",
    "testData.to_csv('./Dataset/testingData', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and shuffle dataset\n",
    "testData = pd.read_csv('./Dataset/testingData').astype('float32')\n",
    "testData = testData.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(testData[testData['Class'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraudSample = testData[testData['Class'] == 1]\n",
    "nfraudSample = testData[testData['Class'] == 0].sample(650)\n",
    "\n",
    "sampleData = pd.concat([fraudSample, nfraudSample])\n",
    "dataY = sampleData[\"Class\"].values\n",
    "dataX = sampleData.drop([\"Class\"], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for TSNE visualization...\n",
    "def tsne_plot(x1, y1):\n",
    "    tsne = TSNE()\n",
    "    X_t = tsne.fit_transform(x1)\n",
    "\n",
    "    plt.scatter(X_t[np.where(y1 == 0), 0], X_t[np.where(y1 == 0), 1], marker='o', color='g', linewidth=1, alpha=0.8, label='Non Fraud')\n",
    "    plt.scatter(X_t[np.where(y1 == 1), 0], X_t[np.where(y1 == 1), 1], marker='o', color='r', linewidth=1, alpha=0.8, label='Fraud')\n",
    "    plt.legend(loc='lower center')\n",
    "    plt.show()\n",
    "    \n",
    "tsne_plot(dataX, dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(tf.keras.Model):\n",
    "    def __init__(self, inputShape, latent_dim):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        self.inputShape = inputShape\n",
    "        self.latent_dim = latent_dim\n",
    "        self.prior = tfp.distributions.MultivariateNormalDiag(loc=tf.zeros(latent_dim))\n",
    "        self.encoder = self.build_encoder()\n",
    "        self.decoder = self.build_decoder()\n",
    "\n",
    "    def build_encoder(self):\n",
    "        encoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.InputLayer(input_shape=self.inputShape, name='encoder_input'),\n",
    "            tf.keras.layers.Dense(20, activation=tf.nn.leaky_relu),\n",
    "            tf.keras.layers.Dense(10, activation=tf.nn.leaky_relu),\n",
    "            tf.keras.layers.Dense(8, activation=tf.nn.leaky_relu),\n",
    "            tf.keras.layers.Dense(4, activation=tf.nn.leaky_relu),\n",
    "            tf.keras.layers.Dense(tfp.layers.MultivariateNormalTriL.params_size(self.latent_dim), activation=None),\n",
    "            tfp.layers.MultivariateNormalTriL(self.latent_dim, \n",
    "                                               activity_regularizer=tfp.layers.KLDivergenceRegularizer(self.prior)),\n",
    "        ], name='encoder')\n",
    "        return encoder\n",
    "\n",
    "    def build_decoder(self):\n",
    "        decoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.InputLayer(input_shape=[self.latent_dim]),\n",
    "            tf.keras.layers.Dense(4, activation=tf.nn.leaky_relu),\n",
    "            tf.keras.layers.Dense(8, activation=tf.nn.leaky_relu),\n",
    "            tf.keras.layers.Dense(10, activation=tf.nn.leaky_relu),\n",
    "            tf.keras.layers.Dense(20, activation=tf.nn.leaky_relu),\n",
    "            tf.keras.layers.Dense(self.inputShape[0], activation=None), \n",
    "        ], name='decoder')\n",
    "        return decoder\n",
    "    \n",
    "    def encode(self, x):\n",
    "        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        batch_size = tf.shape(mean)[0]\n",
    "        eps = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "\n",
    "    def decode(self, z, apply_sigmoid=False):\n",
    "        logits = self.decoder(z)\n",
    "        if apply_sigmoid:\n",
    "            probs = tf.sigmoid(logits)\n",
    "            return probs\n",
    "        return logits\n",
    "\n",
    "    def call(self, x):\n",
    "        mean, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        decoded = self.decode(z)\n",
    "        return decoded\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    z_logvar, z_mean = model.encode(y_true)\n",
    "    kl_divergence_loss = -0.5 * tf.reduce_sum(1 + z_logvar - tf.square(z_mean) - tf.exp(z_logvar), axis=-1)\n",
    "    reconstruction_loss = tf.reduce_sum(tf.square(y_true - y_pred), axis=-1) \n",
    "    loss = tf.reduce_mean(reconstruction_loss + kl_divergence_loss)\n",
    "    return loss\n",
    "\n",
    "# Create an instance of the VAE model\n",
    "inputShape = dataX.shape[1:]\n",
    "latent_dim = 2\n",
    "model = VariationalAutoencoder(inputShape, latent_dim)\n",
    "\n",
    "# Build model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss=loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(tf.zeros((1, *inputShape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('weight.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model.evaluate(dataX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rData = model.call(dataX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate log probability for the test dataset\n",
    "log_prob = -tf.reduce_sum(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(labels=dataX, logits=rData),\n",
    "    axis=-1\n",
    ")\n",
    "\n",
    "# Plot histogram of log probability for nonFraud credit cards\n",
    "plt.hist(log_prob[dataY==0] ,bins=80)\n",
    "plt.xlabel('Log Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Log Probability Distribution')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate log probability for the test dataset\n",
    "log_prob = -tf.reduce_sum(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(labels=dataX, logits=rData),\n",
    "    axis=-1\n",
    ")\n",
    "\n",
    "# Plot histogram of log probability for Fraud credit cards\n",
    "plt.hist(log_prob[dataY==1], bins=80)\n",
    "plt.xlabel('Log Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Log Probability Distribution')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing a threshold\n",
    "log_prob[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(dataY, log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "statistics.mean(thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_auc = auc(recall, precision)\n",
    "pr_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(recall, precision, color='blue', label=f'PR AUC = {pr_auc:.2f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating our model based on a single Threshold vaule\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "threshold = 327.00\n",
    "\n",
    "predictions = (log_prob > threshold)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(dataY, predictions)\n",
    "\n",
    "# Extract TP, TN, FP, FN\n",
    "TN, FP, FN, TP = conf_matrix.ravel()\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(dataY, predictions)\n",
    "precision = precision_score(dataY, predictions)\n",
    "recall = recall_score(dataY, predictions)\n",
    "f1 = f1_score(dataY, predictions)\n",
    "\n",
    "print(\"True Positives (TP) ->\", TP)\n",
    "print(\"True Negatives (TN) ->\", TN)\n",
    "print(\"False Positives (FP) ->\", FP)\n",
    "print(\"False Negatives (FN) ->\", FN)\n",
    "print(\"Accuracy ->\", accuracy)\n",
    "print(\"Precision ->\", precision)\n",
    "print(\"Recall ->\", recall)\n",
    "print(\"F1 Score ->\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Confusion matrix...\n",
    "import seaborn as sns\n",
    "\n",
    "labels = [f\"True Negative \\n\\n {TN}\",F\"False Positive \\n\\n {FP}\",F\"False Negative \\n\\n {FN}\",f\"True Positive \\n\\n {TP}\"]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(conf_matrix, annot=labels, fmt=\"\", cmap=\"Blues\", xticklabels=False, yticklabels=False)\n",
    "\n",
    "plt.xlabel('Predicted values')\n",
    "plt.ylabel('True values')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f22fa19263995c18b642ef2dc6c773af5620bdd311d33ec9bbed9059c9ea237"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
